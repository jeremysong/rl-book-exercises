from abc import abstractmethod

import numpy as np


class ActionReward:
    """
    The reward generated by taking an action
    """

    @abstractmethod
    def get_reward(self, action):
        pass

    @abstractmethod
    def get_optimal_action(self):
        pass

    @abstractmethod
    def reset(self):
        pass


class NormalDistributionReward(ActionReward):
    """
    Generates rewards for each action.
    """

    def __init__(self, num_actions):
        self._num_actions = num_actions
        self._generate_reward_distribution(num_actions)
        # print("true q_a mean: {} with optimal action: {}".format(self._q_a_means, self._optimal_action))

    def _generate_reward_distribution(self, num_actions):
        self._q_a_means = np.random.randn(num_actions)
        self._optimal_action = np.argmax(self._q_a_means)

    def get_reward(self, action):
        """
        Reward distribution is a normal distribution with unit variance, but different mean value.
        :return: a reward for an action
        """
        return np.random.randn() + self._q_a_means[action]

    def get_optimal_action(self):
        return self._optimal_action

    def reset(self):
        self._generate_reward_distribution(self._num_actions)


class RandomWalkActionReward(ActionReward):
    """
    All q_a start out equal and take independent random walks (by adding a normally distributed increment with mean zero
    and standard deviation 0.01 to all the q_a on each step).
    """

    def __init__(self, init_q_a, num_actions):
        self._q_a = [init_q_a] * num_actions
        # count number of steps have been taken
        self._counter = 0

    def get_reward(self, action):
        """
        Add a normal distribution N(0, 0.01 ** 2) to reward on each step.
        :return: the reward
        """
        reward = self._q_a[action]
        variance = 0.01 ** 2 * self._counter
        self._counter += 1
        return np.random.randn() * np.math.sqrt(variance) + reward

    def get_optimal_action(self):
        """
        Does not have an optimal action; all actions are optimal
        :return: None
        """
        return None

    def reset(self):
        self._counter = 0
